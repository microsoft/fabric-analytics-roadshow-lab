{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0215ef5c",
   "metadata": {},
   "source": [
    "# Fabric Analytics Roadshow - Deployment Notebook\n",
    "\n",
    "This notebook deploys all lab assets into your Fabric workspace.\n",
    "\n",
    "## Prerequisites\n",
    "- Access to a Microsoft Fabric workspace\n",
    "- Contributor or Admin permissions in the workspace\n",
    "\n",
    "## What This Notebook Does\n",
    "1. Clones Analytics Roadshow GIT Repo to a temp directory\n",
    "1. Install Pilot version of fabric-cicd library that supports Spark Job Definitions\n",
    "1. Uses fabric-cicd python library to deploy the project to the target workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6704c873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: C:\\Users\\MILESC~1\\AppData\\Local\\Temp\\fabric_roadshow_454yxr3g\n",
      "Cloning https://github.com/microsoft/fabric-analytics-roadshow-lab.git (branch: initial-version-prep)...\n",
      "‚úÖ Successfully cloned repository to: C:\\Users\\MILESC~1\\AppData\\Local\\Temp\\fabric_roadshow_454yxr3g\n",
      "üìÅ Workspace directory: C:\\Users\\MILESC~1\\AppData\\Local\\Temp\\fabric_roadshow_454yxr3g\\workspace\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Create a temporary directory\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"fabric_roadshow_\")\n",
    "print(f\"Created temporary directory: {temp_dir}\")\n",
    "\n",
    "# Clone the specific branch\n",
    "repo_url = \"https://github.com/microsoft/fabric-analytics-roadshow-lab.git\"\n",
    "branch_name = \"initial-version-prep\"\n",
    "\n",
    "print(f\"Cloning {repo_url} (branch: {branch_name})...\")\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"--branch\", branch_name, \"--single-branch\", repo_url, temp_dir],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Successfully cloned repository to: {temp_dir}\")\n",
    "    workspace_root = os.path.join(temp_dir, \"workspace\")\n",
    "    print(f\"üìÅ Workspace directory: {workspace_root}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error cloning repository:\")\n",
    "    print(result.stderr)\n",
    "    raise Exception(\"Failed to clone repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da9de52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing fabric_cicd from: C:\\Users\\MILESC~1\\AppData\\Local\\Temp\\fabric_roadshow_454yxr3g\\setup\\fabric_cicd-0.1.33-py3-none-any.whl\n",
      "‚úÖ Successfully installed fabric_cicd\n"
     ]
    }
   ],
   "source": [
    "# Install fabric_cicd from the cloned repository\n",
    "whl_path = os.path.join(temp_dir, \"setup\", \"fabric_cicd-0.1.33-py3-none-any.whl\")\n",
    "print(f\"Installing fabric_cicd from: {whl_path}\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", whl_path, \"--force-reinstall\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ Successfully installed fabric_cicd\")\n",
    "else:\n",
    "    print(f\"‚ùå Installation failed:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecaf5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1m[info]   16:38:17 - Executing as User 'milescole@microsoft.com'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Lakehouse.SalesAndLogisticsLH.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Lakehouse.SalesAndLogisticsLH.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$workspace.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Lakehouse.SalesAndLogisticsLH.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Lakehouse.SalesAndLogisticsLH.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Environment.SparkEnv.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$workspace.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Lakehouse.SalesAndLogisticsLH.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Lakehouse.SalesAndLogisticsLH.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.Environment.SparkEnv.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The provided is_regex value is not set to 'true', regex matching will be ignored.\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.SparkJobDefinition.StreamBronzeAndSilver.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The provided is_regex value is not set to 'true', regex matching will be ignored.\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$workspace.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The provided is_regex value is not set to 'true', regex matching will be ignored.\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$items.SparkJobDefinition.StreamBronzeAndSilver.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The provided is_regex value is not set to 'true', regex matching will be ignored.\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - Found the reserved environment key '_ALL_'\u001b[0m\n",
      "\u001b[33m[warn]   16:38:17 - The replace value: '$workspace.$id' will be applied for any target environment\u001b[0m\n",
      "\u001b[37m\u001b[1m[info]   16:38:17 - Parameter file validation passed\u001b[0m\n",
      "\u001b[37m\u001b[1m[info]   16:38:17 - Parameter file validation passed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m########## Validating Parameter File ###############################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1m[info]   16:38:18 - Publishing Workspace Folders\u001b[0m\n",
      "         16:38:18 - Published\u001b[0m\n",
      "         16:38:18 - Published\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m########## Publishing Workspace Folders ############################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1m[info]   16:38:19 - Publishing Lakehouse 'SalesAndLogisticsLH'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m########## Publishing Lakehouses ###################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         16:38:24 - SQL Endpoint provisioning in progress Checking again in 10 seconds (Attempt 1)...\u001b[0m\n",
      "         16:38:35 - SQL Endpoint provisioning in progress Checking again in 20 seconds (Attempt 2)...\u001b[0m\n",
      "         16:38:35 - SQL Endpoint provisioning in progress Checking again in 20 seconds (Attempt 2)...\u001b[0m\n",
      "         16:38:56 - SQL Endpoint provisioned successfully\u001b[0m\n",
      "         16:38:56 - Published\u001b[0m\n",
      "\u001b[33m[warn]   16:38:56 - The underlying legacy Microsoft Fabric Environment APIs will be deprecated by March 1, 2026.\u001b[0m\n",
      "\u001b[33m[warn]   16:38:56 - Please upgrade to the latest fabric-cicd version before March 1, 2026 to prevent broken Environment item deployments.\u001b[0m\n",
      "\u001b[37m\u001b[1m[info]   16:38:56 - Checking Environment Publish State for ['SparkEnv']\u001b[0m\n",
      "         16:38:56 - SQL Endpoint provisioned successfully\u001b[0m\n",
      "         16:38:56 - Published\u001b[0m\n",
      "\u001b[33m[warn]   16:38:56 - The underlying legacy Microsoft Fabric Environment APIs will be deprecated by March 1, 2026.\u001b[0m\n",
      "\u001b[33m[warn]   16:38:56 - Please upgrade to the latest fabric-cicd version before March 1, 2026 to prevent broken Environment item deployments.\u001b[0m\n",
      "\u001b[37m\u001b[1m[info]   16:38:56 - Checking Environment Publish State for ['SparkEnv']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m########## Publishing Environments #################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1m[info]   16:38:56 - Publishing Environment 'SparkEnv'\u001b[0m\n",
      "         16:39:00 - Operation in progress. Checking again in 1 second (Attempt 1)...\u001b[0m\n",
      "         16:39:00 - Operation in progress. Checking again in 1 second (Attempt 1)...\u001b[0m\n",
      "         16:39:02 - Updated Spark Settings\u001b[0m\n",
      "         16:39:02 - Updated Spark Settings\u001b[0m\n",
      "         16:39:05 - Publish Submitted for Environment 'SparkEnv'\u001b[0m\n",
      "         16:39:05 - Publish Submitted for Environment 'SparkEnv'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m########## Publishing Spark Job Definitions ########################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1m[info]   16:39:10 - Publishing SparkJobDefinition 'StreamBronzeAndSilver'\u001b[0m\n",
      "         16:39:14 - Operation in progress. Checking again in 1 second (Attempt 1)...\u001b[0m\n",
      "         16:39:14 - Operation in progress. Checking again in 1 second (Attempt 1)...\u001b[0m\n",
      "         16:39:16 - Published\u001b[0m\n",
      "         16:39:16 - Published\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m########## Publishing Notebooks ####################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1m[info]   16:39:23 - Publishing Notebook '1_ExploreData'\u001b[0m\n",
      "         16:39:26 - Published\u001b[0m\n",
      "\u001b[37m\u001b[1m[info]   16:39:26 - Checking Environment Publish State for ['SparkEnv']\u001b[0m\n",
      "         16:39:26 - Published\u001b[0m\n",
      "\u001b[37m\u001b[1m[info]   16:39:26 - Checking Environment Publish State for ['SparkEnv']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m########## Checking Environment Publish State ######################################################\u001b[0m\n",
      "\u001b[32m\u001b[1m####################################################################################################\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         16:39:27 - Operation in progress. Checking again in 10 seconds (Attempt 1)...\u001b[0m\n",
      "         16:39:38 - Operation in progress. Checking again in 20 seconds (Attempt 2)...\u001b[0m\n",
      "         16:39:38 - Operation in progress. Checking again in 20 seconds (Attempt 2)...\u001b[0m\n",
      "         16:39:58 - Operation in progress. Checking again in 40 seconds (Attempt 3)...\u001b[0m\n",
      "         16:39:58 - Operation in progress. Checking again in 40 seconds (Attempt 3)...\u001b[0m\n",
      "         16:40:39 - Operation in progress. Checking again in 80 seconds (Attempt 4)...\u001b[0m\n",
      "         16:40:39 - Operation in progress. Checking again in 80 seconds (Attempt 4)...\u001b[0m\n",
      "         16:42:00 - Operation in progress. Checking again in 120 seconds (Attempt 5)...\u001b[0m\n",
      "         16:42:00 - Operation in progress. Checking again in 120 seconds (Attempt 5)...\u001b[0m\n",
      "         16:44:01 - Operation in progress. Checking again in 120 seconds (Attempt 6)...\u001b[0m\n",
      "         16:44:01 - Operation in progress. Checking again in 120 seconds (Attempt 6)...\u001b[0m\n",
      "         16:46:01 - Published: ['SparkEnv']\u001b[0m\n",
      "         16:46:01 - Published: ['SparkEnv']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items, append_feature_flag\n",
    "\n",
    "append_feature_flag(\"enable_lakehouse_unpublish\")\n",
    "\n",
    "# Sample values for FabricWorkspace parameters\n",
    "workspace_id = \"0ba90db6-964b-4451-9777-d7228c601c0e\"\n",
    "# Use the workspace directory from the cloned repository\n",
    "repository_directory = workspace_root\n",
    "item_type_in_scope = [\"Notebook\", \"Lakehouse\", \"SparkJobDefinition\", \"Environment\"]  # \n",
    "\n",
    "# Initialize the FabricWorkspace object with the required parameters\n",
    "target_workspace = FabricWorkspace(\n",
    "    workspace_id=workspace_id,\n",
    "    repository_directory=repository_directory,\n",
    "    item_type_in_scope=item_type_in_scope,\n",
    "    enable_lakehouse_unpublish=True\n",
    ")\n",
    "\n",
    "# Publish all items defined in item_type_in_scope\n",
    "publish_all_items(target_workspace)\n",
    "\n",
    "# Unpublish all items defined in item_type_in_scope not found in repository\n",
    "#unpublish_all_orphan_items(target_workspace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
