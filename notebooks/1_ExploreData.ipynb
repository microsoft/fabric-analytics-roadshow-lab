{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cb660b",
   "metadata": {},
   "source": [
    "# üöÄ Welcome to the Fabric Analytics Roadshow Lab!\n",
    "\n",
    "## Overview\n",
    "Welcome to the **McMillan Industrial Group** analytics transformation journey! In this lab, you'll build a modern, streaming-enabled data lakehouse using Microsoft Fabric.\n",
    "\n",
    "### The Business Scenario\n",
    "McMillan Industrial Group is a leading manufacturer and distributor of industrial equipment and parts. Their systems generate real-time data from:\n",
    "- üë• **Customers** - Customer master data and profiles\n",
    "- üìù **Orders** - Sales orders placed online and manually\n",
    "- üì¶ **Items** - Item master data\n",
    "- üöö **Shipments** - Outbound shipments and delivery tracking\n",
    "- üì± **Shipment Scan Events** - Real-time package scanning from field technicians and warehouse systems\n",
    "- üåê **Logistics Dimensions** - Facilities, routes, shipping methods, service level, and exception type\n",
    "\n",
    "This data streams continuously into OneLake in various formats (JSON, Parquet), and your mission is to transform raw data into actionable business intelligence.\n",
    "\n",
    "### Architecture: Medallion Pattern\n",
    "We'll implement a **medallion architecture** - a common practice for organizing data based on the level of data refinement and readiness for end-user consumption:\n",
    "\n",
    "> ‚ÑπÔ∏è _Note: similar streaming scenarios ideally leverage Azure Event Hubs or Fabric EventStreams to store events in a message store that manages sequence and provides a simple consumption endpoint. The same JSON payloads could be coming from either of these message stores, however for simplicity of reproducing the use case, we will be reading events as files stored in OneLake._\n",
    "\n",
    "```\n",
    "üì• Landing Zone (Raw Data: JSON/Parquet)\n",
    "    ‚Üì Spark - Structured Streaming\n",
    "ü•â BRONZE Zone - Raw ingestion with audit columns and column name cleaning\n",
    "    ‚Üì Spark - Structured Streaming\n",
    "ü•à SILVER Zone - Cleaned, validated, and conformed data\n",
    "    ‚Üì Fabric Warehouse - Dimensional Modeling\n",
    "ü•á GOLD Zone - Business-level aggregates (Warehouse)\n",
    "    ‚Üì\n",
    "üìä Analytics & AI - Data Agent and Semantic Models\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Lab Setup: Start Your Data Pipeline!\n",
    "\n",
    "Before we explore Spark fundamentals, you need to **start the production-grade streaming pipeline** that will process data throughout this lab.\n",
    "\n",
    "### üìã Step 1: Trigger the Spark Job Definition\n",
    "> _Note: read the instructions for the step before opening the Spark Job Definition_\n",
    "\n",
    "1. **Open Spark Job Definition** open the Spark Job Definition by clicking here [StreamBonzeAndSilver](https://msit.powerbi.com/groups/60c4c0e4-1e55-44cc-b6c3-860d3bb431ba/sparkjobdefinitions/fd65b0b2-98a3-48cd-8b9d-9c35674fe1e5?experience=power-bi)\n",
    "1. **Click** the **\"Run\"** button at the top of the screen\n",
    "1. **Confirm** the job starts successfully (you'll see a status of \"Running\")\n",
    "1. **Return** to this Notebook (1_ExploreData)\n",
    "\n",
    "### ‚è±Ô∏è What Happens Next?\n",
    "\n",
    "The Spark Job Definition you just triggered will:\n",
    "- üé≤ **Generate synthetic data** simulating McMillan's business operations\n",
    "- üìù **Write JSON and Parquet files** to the Landing zone (folder) of your Lakehouse\n",
    "- ‚ö° **Stream data** from Landing ‚Üí Bronze ‚Üí Silver zones\n",
    "- üîÑ **Run continuously** for the duration of this lab\n",
    "\n",
    "> üí° **Pro Tip**: The job runs in the background. You don't need to wait for it to complete - you can start working through this notebook immediately! The job should take ~ 1.5 minutes to start writing data to `Files/landing/` and another 2-3 minutes for all bronze and silver tables to be intially created and hydrated with data.\n",
    "\n",
    "### What You'll Learn in This Notebook\n",
    "\n",
    "1. **Spark Fundamentals** - DataFrames, transformations, and actions\n",
    "1. **Structured Streaming** - Processing real-time and batch data with Spark\n",
    "1. **Data Exploration** - Discover what's already been processed in Bronze & Silver zones\n",
    "\n",
    "### The Target Schema\n",
    "By the end of the lab, you'll understand some basic concepts and then see the outcome of a mature data engineering pipeline:\n",
    "\n",
    "![McMillian Industrial Group Silver Schema](https://github.com/microsoft/fabric-analytics-roadshow-lab/blob/initial-version-prep/assets/images/spark/silver-erd.png?raw=true)\n",
    "\n",
    "Let's get started! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03c938",
   "metadata": {},
   "source": [
    "## üìö Part 1: Spark Fundamentals\n",
    "\n",
    "Before diving into streaming data, let's understand the power of Apache Spark. Spark is a distributed computing engine that allows you to process massive datasets across one or many of machines.\n",
    "\n",
    "### Key Concepts\n",
    "- **DataFrame**: A distributed collection of data organized into named columns (like a table)\n",
    "- **Lazy Evaluation**: Transformations aren't executed until an action is called\n",
    "- **Partitioning**: Data is split across multiple nodes for parallel processing\n",
    "- **In-Memory Processing**: Spark caches data in RAM for lightning-fast analysis\n",
    "\n",
    "Fabric Spark Notebooks have a Spark session already stated, so lets get right into exploring some data!\n",
    "\n",
    "Execute the below cell to preview parquet data landing in the `Files/landing/item` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file via Spark\n",
    "df = spark.read.parquet('Files/landing/item')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de9e99",
   "metadata": {},
   "source": [
    "Run the below cell to preview JSON data from the `Files/landing/shipment` folder. Notice how there's a `data` `Struct` column. This contains the entire shipment structure with various nested elements. This data will be flattened when writing to the Silver zone.\n",
    "\n",
    "> ‚ÑπÔ∏è Complex data type columns (Struct, Map, Array, etc.) can be drilled into via clicking on a cell value and then clicking the carrot symbol \n",
    "\n",
    "![Explore Struct](https://github.com/microsoft/fabric-analytics-roadshow-lab/blob/initial-version-prep/assets/images/spark/explore-struct.gif?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file via Spark\n",
    "df = spark.read.json('Files/landing/shipment', multiLine=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755acba",
   "metadata": {},
   "source": [
    "### üîÑ Switching Between DataFrame API and Spark SQL\n",
    "\n",
    "While the PySpark DataFrame API was just used to preview the two files, we can also use **Spark SQL** to query and explore the same data using familiar SQL syntax. Both approaches are equally powerful and often interchangeable.\n",
    "\n",
    "In the next cell, we'll demonstrate two key SQL concepts:\n",
    "\n",
    "1. **Creating a Temporary View**: We register the JSON files as a SQL table that exists only for the duration of this session. This allows us to query file-based data as if it were a database table. While we could do a simple `SELECT * FROM json.'<file_path>'` in many scenarios, we are using a temp view to allow expressing additional options such as the `multiLine` json configuration.\n",
    "\n",
    "1. **Exploding Nested Arrays**: The shipment JSON contains an array of shipment records. We use the `EXPLODE()` function to transform this array into individual rows, then use `*` to expand all columns from the nested struct into a flat table structure.\n",
    "\n",
    "This flattening pattern is essential when working with semi-structured data and will be a core pattern to prepare data for dimensional modeling.\n",
    "\n",
    "> üí° **Pro Tip**: Use the `%%sql` magic command at the top of a cell or `spark.sql()` to write pure SQL instead of PySpark code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71739ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_data\n",
    "USING JSON\n",
    "OPTIONS (\n",
    "  path \"Files/landing/shipment\",\n",
    "  multiLine \"true\"\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6822475",
   "metadata": {},
   "source": [
    "üìå **Challenge:** Write a `SELECT` statement to query the `shipment_data` temporary view.\n",
    "\n",
    "Tips:\n",
    "- you can use the `explode(<column_name>)` function to explode an array of values.\n",
    "- use star expand, `<column_name>.*`, to select all top level elements in a struct or map column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e287f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><strong>üîë Example Answer:</strong>  Click to see an example query that explodes and expands the struct columns.</summary>\n",
    "\n",
    "~~~sql\n",
    "SELECT shipment.*\n",
    "FROM (\n",
    "    SELECT explode(data) as shipment FROM shipment_data\n",
    "    );\n",
    "~~~\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f7628",
   "metadata": {},
   "source": [
    "---\n",
    "## üåä Part 2: Why Structured Streaming?\n",
    "\n",
    "**Structured Streaming** is Spark's powerful engine for processing data streams, but it's useful far beyond just real-time, low-latency scenarios. Here's why it's commonly used in modern data engineering:\n",
    "\n",
    "### üéØ Key Benefits\n",
    "\n",
    "1. **‚úÖ Built-in Incremental Processing**\n",
    "   - Automatically tracks which data has been processed\n",
    "   - Only processes new/changed files since the last run\n",
    "   - No need to manually manage watermarks or state\n",
    "\n",
    "1. **‚úÖ Exactly-Once Semantics**\n",
    "   - Guarantees each record is processed exactly once\n",
    "   - Prevents duplicate data in your Delta tables\n",
    "   - Handles failures gracefully with automatic recovery\n",
    "\n",
    "1. **‚úÖ Fault Tolerance**\n",
    "   - Checkpointing saves progress automatically\n",
    "   - If a job fails, it resumes from the last checkpoint\n",
    "   - No data loss or reprocessing of already-handled records\n",
    "\n",
    "1. **‚úÖ Unified API**\n",
    "   - Same DataFrame API for batch and streaming\n",
    "   - Write once, run in batch or streaming mode\n",
    "   - Easy to prototype in batch, deploy as streaming\n",
    "\n",
    "1. **‚úÖ Optimized for Delta Lake**\n",
    "   - Native integration with Delta tables\n",
    "   - Handles schema evolution automatically\n",
    "   - Enables time travel and data versioning\n",
    "\n",
    "### üíº Common Use Cases\n",
    "\n",
    "- **ETL Pipelines**: Continuously ingest and transform data as it arrives\n",
    "- **Data Lakehouse**: Build incremental Bronze ‚Üí Silver ‚Üí Gold pipelines\n",
    "- **Real-time Analytics**: Power dashboards with up-to-the-minute data\n",
    "- **Change Data Capture (CDC)**: Process CDC data from source systems\n",
    "- **Event Processing**: Handle IoT sensors, clickstreams, logs, etc.\n",
    "\n",
    "### üèóÔ∏è Architecture in This Lab\n",
    "\n",
    "In our medallion architecture, Structured Streaming powers:\n",
    "- **Landing ‚Üí Bronze**: Ingesting raw JSON/Parquet files with audit metadata and column renaming (snake case)\n",
    "- **Bronze ‚Üí Silver**: Flattening nested structures, applying business rules, data quality checks\n",
    "\n",
    "Even though the data arrives as files in OneLake (not a traditional message store), Structured Streaming gives us:\n",
    "- ‚úÖ Incremental processing (only new files)\n",
    "- ‚úÖ Exactly-once guarantees (no duplicates)\n",
    "- ‚úÖ Automatic restart capability (fault tolerance)\n",
    "- ‚úÖ Scalability (handles growing data volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89335619",
   "metadata": {},
   "source": [
    "---\n",
    "## üåä Part 3: Structured Streaming Fundamentals\n",
    "\n",
    "Structured Streaming is Spark's scalable and fault-tolerant stream processing engine. It treats streaming data as an unbounded table that grows continuously.\n",
    "\n",
    "### Key Streaming Concepts\n",
    "\n",
    "1. **Input Source**: Where data comes from (files, Kafka, Event Hubs, etc.)\n",
    "1. **Transformations**: How you process each micro-batch\n",
    "1. **Output Sink**: Where results are written (Delta tables, console, etc.)\n",
    "1. **Checkpointing**: Tracks progress for fault tolerance\n",
    "1. **Trigger Intervals**: How often to process new data\n",
    "\n",
    "### The Streaming Pattern\n",
    "\n",
    "```python\n",
    "# Read stream from source\n",
    "df = spark.readStream.format(\"json\").load(\"path/to/input\")\n",
    "\n",
    "# Apply transformations\n",
    "transformed = df.select(...).where(...).withColumn(...)\n",
    "\n",
    "# Write to Delta Lake\n",
    "query = transformed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint\") \\\n",
    "    .start(\"path/to/delta/table\")\n",
    "```\n",
    "\n",
    "Let's create a simple streaming example to understand these concepts! üëá\n",
    "\n",
    "First, let's revisit querying the folder that contains `item` parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = spark.read.parquet('Files/landing/item')\n",
    "display(item_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349f912",
   "metadata": {},
   "source": [
    "After validating the output, we can simply switch to the `readStream` API, specify the schema of the input data, and configure our write settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271156be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame to incrementally read only new parquet files as they arrive\n",
    "item_stream_df = spark.readStream.schema(item_df.schema).parquet('Files/landing/item')\n",
    "\n",
    "# Write stream triggered as a single batch (process available files)\n",
    "item_stream = (item_stream_df.writeStream\n",
    "    .format('delta')\n",
    "    .outputMode('append')\n",
    "    .option('checkpointLocation', 'Files/test/checkpoints/item')\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable('dbo.item')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364d89e",
   "metadata": {},
   "source": [
    "Streaming jobs can be triggered as `syncronous` or `asyncronous` operations depending on design requirements.\n",
    "\n",
    "To check the status of the `async` streaming job run `<stream_variable_name>.status` for overall status and `<stream_variable_name>.lastProcess` for stats about the last completed batch. \n",
    "\n",
    "Note: to await the termination of the streaming job, a.k.a. syncronous execution, call `<stream_variable_name>.awaitTerimination()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87791729",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stream.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdea78",
   "metadata": {},
   "source": [
    "One the `status` message shows as **Stopped** or the `lastProgress` dictionary returns metrics, browse the `dbo` schema of the Lakehouse object explorer on the left to verify the item table was created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2d2db",
   "metadata": {},
   "source": [
    "Now let's explore the data produced by the triggered Spark Job Definition to see the data currently being streamed into Bronze and Silver zones!\n",
    "Run the below to get a count of `shipment_scan_event` records processed through to the `silver` zone. üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ceaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(1) FROM silver.shipment_scan_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94cd11",
   "metadata": {},
   "source": [
    "Now query the `silver.shipment` table to see how the nested structures that we previously viewed are flattened: üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * from silver.shipment LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2b42f",
   "metadata": {},
   "source": [
    "Now lets check the latency from IoT device to Delta table in our bronze zone:\n",
    "\n",
    "> Tip: click `New chart` to visualize the latency by data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ac8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT data.generated_at, _processing_timestamp, (unix_millis(_processing_timestamp) - unix_millis(cast(data.generated_at as timestamp))) / 1000.0 AS seconds_latency_from_source \n",
    "FROM bronze.shipment_scan_event\n",
    "group by all\n",
    "order by cast(data.generated_at as timestamp) desc LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2177b6",
   "metadata": {},
   "source": [
    "Execute the below SparkSQL to see the latency from source device to the `silver` zone (data flows from landing -> bronze -> silver): üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9936256",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT generated_at, _processing_timestamp, (unix_millis(_processing_timestamp) - unix_millis(generated_at)) / 1000.0 AS seconds_latency_from_source \n",
    "FROM silver.shipment_scan_event\n",
    "group by all\n",
    "order by generated_at desc LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3220cc",
   "metadata": {},
   "source": [
    "Our streaming data is now parsed, cleaned, and ready for dimensional modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f840f73",
   "metadata": {},
   "source": [
    "---\n",
    "## ü•â Part 4: Exploring Bronze Zone Data\n",
    "\n",
    "The **Bronze zone** is where raw data lands with minimal transformation. It preserves the original format and includes metadata about ingestion.\n",
    "\n",
    "### Bronze Zone Characteristics\n",
    "- ‚úÖ Schema-on-read approach\n",
    "- ‚úÖ Preserves raw JSON/Parquet structure\n",
    "- ‚úÖ Includes ingestion metadata (timestamp, source file, etc.)\n",
    "- ‚úÖ Serves as historical archive\n",
    "- ‚úÖ Enables data lineage and debugging\n",
    "\n",
    "Let's see what the streaming job has already processed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0232ca2",
   "metadata": {},
   "source": [
    "---\n",
    "## ü•à Part 5: Exploring Silver Zone Data\n",
    "\n",
    "The **Silver zone** contains cleaned, validated, and conformed data. This is where data quality rules are applied and semi-structured data is flattened into structured tables.\n",
    "\n",
    "### Silver Zone Characteristics\n",
    "- ‚úÖ Clean, validated data (nulls handled, types enforced)\n",
    "- ‚úÖ Flattened JSON structures\n",
    "- ‚úÖ Standardized column names\n",
    "- ‚úÖ Business rules applied\n",
    "- ‚úÖ Ready for dimensional modeling\n",
    "\n",
    "This is the data that analysts and data scientists will work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebf747",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Part 6: Key Takeaways & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. ‚úÖ **Spark Fundamentals**\n",
    "   - DataFrames and transformations\n",
    "   - Lazy evaluation and optimization\n",
    "   - Aggregations and window functions\n",
    "\n",
    "1. ‚úÖ **Structured Streaming**\n",
    "   - Reading streaming data sources\n",
    "   - Applying transformations in real-time\n",
    "   - Writing to Delta Lake sinks\n",
    "\n",
    "1. ‚úÖ **Medallion Architecture**\n",
    "   - Bronze zone: Raw data preservation\n",
    "   - Silver zone: Cleaned and conformed data\n",
    "   - Understanding data quality at each layer\n",
    "\n",
    "1. ‚úÖ **Data Exploration**\n",
    "   - Analyzing table schemas and relationships\n",
    "   - Calculating business metrics\n",
    "   - Validating data quality\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the subsequent notebooks, you'll:\n",
    "\n",
    "1. **ü•á Gold Layer** - Create dimensional models in Fabric Warehouse\n",
    "1. **üìä Analytics** - Use a Data Agent to answer questions about your data\n",
    "\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Microsoft Fabric Documentation](https://learn.microsoft.com/fabric/)\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Delta Lake Best Practices](https://docs.delta.io/latest/best-practices.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** Move on to the next notebook to start building your streaming pipelines! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
