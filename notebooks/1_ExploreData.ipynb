{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cb660b",
   "metadata": {},
   "source": [
    "![](https://github.com/microsoft/fabric-analytics-roadshow-lab/blob/initial-version-prep/assets/images/spark/analytics.png?raw=true)\n",
    "# Welcome to the Fabric Analytics Roadshow Lab\n",
    "\n",
    "## Overview\n",
    "Welcome to the **McMillan Industrial Group** analytics transformation journey! In this lab, you'll build a modern, streaming-enabled data lakehouse using Microsoft Fabric.\n",
    "\n",
    "### The Business Scenario\n",
    "McMillan Industrial Group is a leading manufacturer and distributor of industrial equipment and parts. Their systems generate real-time data from:\n",
    "- üë• **Customers** - Customer master data and profiles\n",
    "- üìù **Orders** - Sales orders placed online and manually\n",
    "- ‚öôÔ∏è **Items** - Item master data\n",
    "- üì¶ **Shipments** - Outbound shipments and delivery tracking\n",
    "- üì± **Shipment Scan Events** - Real-time package scanning from field technicians and warehouse systems\n",
    "- üöö **Logistics Dimensions** - Facilities, routes, shipping methods, service level, and exception type\n",
    "\n",
    "This data streams continuously into OneLake in various formats (JSON, Parquet), and your mission is to transform raw data into actionable business intelligence.\n",
    "\n",
    "### Architecture: Medallion Pattern\n",
    "We'll implement a **medallion architecture** - a common practice for organizing data based on the level of data refinement and readiness for end-user consumption:\n",
    "\n",
    "> ‚ÑπÔ∏è _Note: similar streaming scenarios ideally leverage Azure Event Hubs or Fabric EventStreams to store events in a message store that manages sequence and provides a simple consumption endpoint. The same JSON payloads could be coming from either of these message stores, however for simplicity of reproducing the use case, we will be reading events as files stored in OneLake._\n",
    "\n",
    "```\n",
    "üì• Landing Zone (Raw Data: JSON/Parquet)\n",
    "    ‚Üì Spark - Structured Streaming\n",
    "ü•â BRONZE Zone - Raw ingestion with audit columns and column name cleaning\n",
    "    ‚Üì Spark - Structured Streaming\n",
    "ü•à SILVER Zone - Cleaned, validated, and conformed data\n",
    "    ‚Üì Fabric Warehouse - Dimensional Modeling\n",
    "ü•á GOLD Zone - Business-level aggregates (Warehouse)\n",
    "    ‚Üì\n",
    "ü§ñ Analytics & AI - Data Agent and Semantic Models\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Lab Setup: Start Your Data Pipeline!\n",
    "\n",
    "Before we explore Spark fundamentals, you need to **start the production-grade streaming pipeline** that will process data throughout this lab.\n",
    "\n",
    "### Step 1: Trigger the Spark Job Definition\n",
    "> **Note:** Please read the full instructions for this step before opening the Spark Job Definition.\n",
    "\n",
    "1. **Open Spark Job Definition** - Click here to open: [StreamBonzeAndSilver](https://msit.powerbi.com/groups/60c4c0e4-1e55-44cc-b6c3-860d3bb431ba/sparkjobdefinitions/fd65b0b2-98a3-48cd-8b9d-9c35674fe1e5?experience=power-bi)\n",
    "1. **Click** the **\"Run\"** button at the top of the screen\n",
    "1. **Confirm** the job starts successfully (you'll see a status of \"Running\")\n",
    "1. **Return** to this Notebook (1_ExploreData)\n",
    "\n",
    "### What Happens Next\n",
    "\n",
    "The Spark Job Definition you just triggered will:\n",
    "- üé≤ **Generate synthetic data** simulating McMillan's business operations\n",
    "- üìù **Write JSON and Parquet files** to the Landing zone (folder) of your Lakehouse\n",
    "- ‚ö° **Stream data** from Landing ‚Üí Bronze ‚Üí Silver zones\n",
    "- üîÑ **Run continuously** for the duration of this lab\n",
    "\n",
    "> ‚ÑπÔ∏è **Important:** The job runs in the background. You don't need to wait for it to complete - you can start working through this notebook immediately. The job should take approximately 1.5 minutes to start writing data to `Files/landing/` and another 2-3 minutes for all bronze and silver tables to be initially created and hydrated with data.\n",
    "\n",
    "### What You'll Learn in This Notebook\n",
    "\n",
    "1. **Spark Fundamentals** - DataFrames, transformations, and actions\n",
    "2. **Structured Streaming** - Processing real-time and batch data with Spark\n",
    "3. **Data Exploration** - Discover what's already been processed in Bronze & Silver zones\n",
    "\n",
    "### The Target Schema\n",
    "By the end of the lab, you'll understand some basic concepts and then see the outcome of a mature data engineering pipeline:\n",
    "\n",
    "![McMillian Industrial Group Silver Schema](https://github.com/microsoft/fabric-analytics-roadshow-lab/blob/initial-version-prep/assets/images/spark/silver-erd.png?raw=true)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03c938",
   "metadata": {},
   "source": [
    "## üìö Part 1: Spark Fundamentals\n",
    "\n",
    "Before diving into streaming data, let's understand the power of Apache Spark. Spark is a distributed computing engine that allows you to process massive datasets across one or many machines.\n",
    "\n",
    "### Key Concepts\n",
    "- **DataFrame**: A distributed collection of data organized into named columns (like a table)\n",
    "- **Lazy Evaluation**: Transformations aren't executed until an action is called\n",
    "- **Partitioning**: Data is split across multiple nodes for parallel processing\n",
    "- **In-Memory Processing**: Spark caches data in RAM for lightning-fast analysis\n",
    "\n",
    "Fabric Spark Notebooks have a Spark session already started, so let's get right into exploring some data.\n",
    "\n",
    "Execute the cell below to preview parquet data landing in the `Files/landing/item` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet via Spark\n",
    "df = spark.read.parquet('Files/landing/item')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de9e99",
   "metadata": {},
   "source": [
    "Run the cell below to preview JSON data from the `Files/landing/shipment` folder. Notice how there's a `data` `Struct` column. This contains the entire shipment structure with various nested elements. This data will be flattened when writing to the Silver zone.\n",
    "\n",
    "> ‚ÑπÔ∏è **Tip:** Complex data type columns (Struct, Map, Array, etc.) can be drilled into by clicking on a cell value and then clicking the caret symbol. \n",
    "\n",
    "![Explore Struct](https://github.com/microsoft/fabric-analytics-roadshow-lab/blob/initial-version-prep/assets/images/spark/explore-struct.gif?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON via Spark\n",
    "df = spark.read.json('Files/landing/shipment', multiLine=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755acba",
   "metadata": {},
   "source": [
    "### Switching Between DataFrame API and Spark SQL\n",
    "\n",
    "While the PySpark **DataFrame API** was just used to preview files, we can also use **Spark SQL** to query the same data using familiar SQL syntax. Both approaches are equally powerful and often interchangeable.\n",
    "\n",
    "#### üìã What We'll Demonstrate\n",
    "\n",
    "The next cells show two key SQL patterns:\n",
    "\n",
    "**1. Creating a Temporary View**\n",
    "- Register JSON files as a SQL table (exists only for this session)\n",
    "- Query file-based data as if it were a database table\n",
    "- Express additional options like `multiLine` JSON configuration\n",
    "\n",
    "**2. Exploding Nested Arrays** _(you'll write this query!)_\n",
    "- The shipment JSON contains an **array** of shipment records\n",
    "- Use `EXPLODE()` to transform arrays into individual rows\n",
    "- Use `*` to expand all columns from nested structs into flat columns\n",
    "\n",
    "> üéØ **Why This Matters:** While many data engineers prefer the PySpark DataFrame API, Spark supports SQL too (SparkSQL). It's often easier to express complex business logic in SQL - there's no need to compromise, work in the language that you are most comfortable with! \n",
    "\n",
    "> üí° **Pro Tip**: Use `%%sql` magic command or `spark.sql()` to write pure SparkSQL instead of PySpark code!\n",
    "\n",
    "First, let's create the temporary view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71739ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_data\n",
    "USING JSON\n",
    "OPTIONS (\n",
    "  path \"Files/landing/shipment\",\n",
    "  multiLine \"true\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6822475",
   "metadata": {},
   "source": [
    "### üéØ Challenge: Query Nested JSON Data\n",
    "\n",
    "Now it's your turn! Write a `SELECT` statement to query the `shipment_data` temporary view and flatten the nested structure.\n",
    "\n",
    "**üí° Hints:**\n",
    "- Use `explode(<column_name>)` to expand an array into individual rows\n",
    "- Use `<column_name>.*` to select all top-level elements from a struct or map\n",
    "- You'll need a subquery to explode first, then expand the struct\n",
    "\n",
    "Try it in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e287f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<details>\n",
    "  <summary><strong>üîë Solution:</strong> Click to reveal the answer</summary>\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Approach:**\n",
    "1. Inner query: `EXPLODE(data)` converts the array into rows with alias `shipment`\n",
    "2. Outer query: `shipment.*` expands all struct fields into columns\n",
    "\n",
    "```sql\n",
    "SELECT shipment.*\n",
    "FROM (\n",
    "    SELECT explode(data) as shipment \n",
    "    FROM shipment_data\n",
    ");\n",
    "```\n",
    "\n",
    "**Key Takeaway:** This two-step pattern (explode ‚Üí expand) is fundamental for flattening nested JSON in data engineering pipelines.\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f7628",
   "metadata": {},
   "source": [
    "---\n",
    "## üåä Part 2: Why Structured Streaming?\n",
    "\n",
    "**Structured Streaming** is Spark's powerful engine for processing data streams, but it's useful far beyond just real-time, low-latency scenarios. Here's why it's commonly used in modern data engineering:\n",
    "\n",
    "### üéØ Key Benefits\n",
    "\n",
    "1. **Built-in Incremental Processing**\n",
    "   - Automatically tracks which data has been processed\n",
    "   - Only processes new/changed files since the last run\n",
    "   - No need to manually manage watermarks or state\n",
    "\n",
    "1. **Exactly-Once Semantics**\n",
    "   - Guarantees each record is processed exactly once\n",
    "   - Prevents duplicate data in your Delta tables\n",
    "   - Handles failures gracefully with automatic recovery\n",
    "\n",
    "1. **Fault Tolerance**\n",
    "   - Checkpointing saves progress automatically\n",
    "   - If a job fails, it resumes from the last checkpoint\n",
    "   - No data loss or reprocessing of already-handled records\n",
    "\n",
    "1. **Unified API**\n",
    "   - Same DataFrame API for batch and streaming\n",
    "   - Write once, run in batch or streaming mode\n",
    "   - Easy to prototype in batch, deploy as streaming\n",
    "\n",
    "1. **Optimized for Delta Lake**\n",
    "   - Native integration with Delta tables\n",
    "   - Handles schema evolution automatically\n",
    "   - Enables time travel and data versioning\n",
    "\n",
    "### üíº Common Use Cases\n",
    "\n",
    "- **ETL Pipelines**: Continuously ingest and transform data as it arrives\n",
    "- **Data Lakehouse**: Build incremental Bronze ‚Üí Silver ‚Üí Gold pipelines\n",
    "- **Real-time Analytics**: Power dashboards with up-to-the-minute data\n",
    "- **Change Data Capture (CDC)**: Process CDC data from source systems\n",
    "- **Event Processing**: Handle IoT sensors, clickstreams, logs, etc.\n",
    "\n",
    "### üèóÔ∏è Architecture in This Lab\n",
    "\n",
    "In our medallion architecture, Structured Streaming powers:\n",
    "- **Landing ‚Üí Bronze**: Ingesting raw JSON/Parquet files with audit metadata and column renaming (snake case)\n",
    "- **Bronze ‚Üí Silver**: Flattening nested structures, applying business rules, data quality checks\n",
    "\n",
    "Even though the data arrives as files in OneLake (not a traditional message store), Structured Streaming gives us:\n",
    "- Incremental processing (only new files)\n",
    "- Exactly-once guarantees (no duplicates)\n",
    "- Automatic restart capability (fault tolerance)\n",
    "- Scalability (handles growing data volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89335619",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåä Part 3: Structured Streaming Fundamentals\n",
    "\n",
    "Structured Streaming is Spark's **scalable and fault-tolerant** stream processing engine. It treats streaming data as an **unbounded table** that grows continuously.\n",
    "\n",
    "### üß© Key Streaming Concepts\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Input Source** | Where data comes from (files, Kafka, Event Hubs, etc.) |\n",
    "| **Transformations** | How you process each micro-batch (same API as batch!) |\n",
    "| **Output Sink** | Where results are written (Delta tables, console, memory, etc.) |\n",
    "| **Checkpointing** | Tracks progress for exactly-once processing and fault tolerance |\n",
    "| **Trigger Intervals** | How often to process new data (continuous, fixed interval, available now) |\n",
    "\n",
    "### üîß The Streaming Pattern\n",
    "\n",
    "```python\n",
    "# 1. Read stream from source\n",
    "df = spark.readStream.format(\"json\").load(\"path/to/input\")\n",
    "\n",
    "# 2. Apply transformations (same as batch!)\n",
    "transformed = df.select(...).where(...).withColumn(...)\n",
    "\n",
    "# 3. Write to Delta Lake\n",
    "query = transformed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint\") \\\n",
    "    .start(\"path/to/delta/table\")\n",
    "```\n",
    "\n",
    "### üí° Batch vs Streaming: Same Code!\n",
    "\n",
    "The beauty of Structured Streaming is that **the same transformation code** works for both batch and streaming. The only difference is:\n",
    "- Batch: `spark.read...` ‚Üí `df.write...`\n",
    "- Streaming: `spark.readStream...` ‚Üí `df.writeStream...`\n",
    "\n",
    "Let's see this in action! First, let's query the `item` parquet files using **batch** processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = spark.read.parquet('Files/landing/item')\n",
    "display(item_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349f912",
   "metadata": {},
   "source": [
    "### üîÑ Convert Batch to Streaming\n",
    "\n",
    "After validating the output, we can **simply switch** to the `readStream` API! The transformation logic remains identical, except for the need to specify the schema of the input DataFrame.\n",
    "\n",
    "**Key Changes:**\n",
    "1. `spark.read` ‚Üí `spark.readStream`\n",
    "1. Schema can be implicit or defined ‚Üí `.schema()` is required for streaming operations\n",
    "1. `df.write` ‚Üí `df.writeStream` (add checkpoint location and trigger)\n",
    "\n",
    "Execute the cell below to create your first streaming pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271156be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame to incrementally read only new parquet files as they arrive\n",
    "item_stream_df = spark.readStream.schema(item_df.schema).parquet('Files/landing/item')\n",
    "\n",
    "# Write stream triggered as a single batch (process available files)\n",
    "item_stream = (item_stream_df.writeStream\n",
    "    .format('delta')\n",
    "    .outputMode('append')\n",
    "    .option('checkpointLocation', 'Files/test/checkpoints/item')\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable('dbo.item')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364d89e",
   "metadata": {},
   "source": [
    "### üìä Monitoring Streaming Jobs\n",
    "\n",
    "Streaming jobs can be triggered as **synchronous** or **asynchronous** operations depending on your design requirements.\n",
    "\n",
    "**üîç Check Async Job Status:**\n",
    "\n",
    "| Method | What It Shows |\n",
    "|--------|---------------|\n",
    "| `<stream>.status` | Overall job status (active, stopped, etc.) |\n",
    "| `<stream>.lastProgress` | Detailed metrics about the last completed batch |\n",
    "| `<stream>.awaitTermination()` | Wait for job completion (synchronous execution) |\n",
    "\n",
    "Let's check the status of our streaming job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87791729",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stream.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdea78",
   "metadata": {},
   "source": [
    "### ‚úÖ Verify Your First Streaming Pipeline\n",
    "\n",
    "Once the `status` shows as **\"Stopped\"** or `lastProgress` returns metrics, your streaming job has completed!\n",
    "\n",
    "**üìÇ Verify the Table Was Created:**\n",
    "1. Look at the **Lakehouse explorer** on the left sidebar\n",
    "2. Expand the **Tables** section\n",
    "3. Find the `item` table under the `dbo` schema\n",
    "4. Right-click and select **Load data -> Spark**, drag and drop the table onto your Notebook, or query it with SparkSQL!\n",
    "\n",
    "> üéâ **Congratulations!** You've just created your first Spark Structured Streaming pipeline in Microsoft Fabric!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2d2db",
   "metadata": {},
   "source": [
    "## üé¨ Exploring Your Production Pipeline Data\n",
    "\n",
    "Now let's explore the data produced by the **Spark Job Definition** you triggered earlier! It's been streaming data into Bronze and Silver zones while you've been learning Spark fundamentals.\n",
    "\n",
    "### üìä Silver Zone: Shipment Scan Events\n",
    "\n",
    "Run the cell below to count `shipment_scan_event` records processed to the **Silver zone**:\n",
    "\n",
    "> üîÑ **Try This**: Run this cell multiple times over the next few minutes - watch the count grow as the streaming job processes more data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ceaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(1) FROM silver.shipment_scan_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94cd11",
   "metadata": {},
   "source": [
    "### üì¶ Silver Zone: Flattened Shipment Data\n",
    "\n",
    "Remember the nested JSON structure you saw earlier in `Files/landing/shipment`? Let's see how it's been **flattened** in the Silver zone!\n",
    "\n",
    "Query the `silver.shipment` table below to compare:\n",
    "\n",
    "**What to Notice:**\n",
    "- All nested fields are now **top-level columns** (easier to query!)\n",
    "- Clean, standardized **snake_case** column names\n",
    "- Data types properly enforced (timestamps, numbers, strings)\n",
    "- Ready for joining with other tables and analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * from silver.shipment LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2b42f",
   "metadata": {},
   "source": [
    "### Measure End-to-End Latency: ü•â Bronze Zone\n",
    "\n",
    "Let's measure the **latency** from when scan events are generated at IoT devices to when they land in the **Bronze** Delta table.\n",
    "\n",
    "**Latency Calculation:**\n",
    "```\n",
    "Latency = Processing Timestamp - Device Generated Timestamp\n",
    "```\n",
    "\n",
    "> üí° **Visualization Tip**: After running the query, click **\"New chart\"** in the results to visualize latency trends over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ac8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT data.generated_at, _processing_timestamp, (unix_millis(_processing_timestamp) - unix_millis(cast(data.generated_at as timestamp))) / 1000.0 AS seconds_latency_from_source \n",
    "FROM bronze.shipment_scan_event\n",
    "group by all\n",
    "order by cast(data.generated_at as timestamp) desc LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2177b6",
   "metadata": {},
   "source": [
    "### Measure End-to-End Latency: ü•à Silver Zone\n",
    "\n",
    "Now let's measure latency to the **Silver zone** - this shows the complete journey through your medallion architecture.\n",
    "\n",
    "**Data Flow:**\n",
    "```\n",
    "Device ‚Üí Landing ‚Üí Bronze ‚Üí Silver\n",
    "```\n",
    "\n",
    "This latency includes:\n",
    "- File landing in OneLake\n",
    "- Bronze zone processing (ingestion + metadata)\n",
    "- Silver zone processing (flattening + transformations + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9936256",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT generated_at, _processing_timestamp, (unix_millis(_processing_timestamp) - unix_millis(generated_at)) / 1000.0 AS seconds_latency_from_source \n",
    "FROM silver.shipment_scan_event\n",
    "group by all\n",
    "order by generated_at desc LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3220cc",
   "metadata": {},
   "source": [
    "### ‚úÖ Silver Zone Complete\n",
    "\n",
    "**What You've Accomplished:**\n",
    "\n",
    "Your streaming data is now:\n",
    "- **Parsed** from complex JSON structures\n",
    "- **Cleaned** with standardized naming and types\n",
    "- **Ready** for dimensional modeling in the Gold layer\n",
    "\n",
    "The Silver zone is where the magic happens - raw, messy data transformed into analytics-ready tables that business users can trust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebf747",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Part 6: Key Takeaways & Next Steps\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "\n",
    "Congratulations! You've explored a production-grade streaming data pipeline in Microsoft Fabric. Here's what you've learned:\n",
    "\n",
    "#### 1. Spark Fundamentals\n",
    "- **DataFrame API**: Reading Parquet and JSON files\n",
    "- **Spark SQL**: Using `%%sql` magic commands and temporary views\n",
    "- **Data Exploration**: Previewing schemas and nested structures\n",
    "- **Batch vs Streaming**: Understanding the unified API\n",
    "\n",
    "#### 2. Structured Streaming in Action\n",
    "- **Incremental Processing**: Only new files are processed automatically\n",
    "- **Checkpointing**: Fault-tolerant, exactly-once semantics\n",
    "- **Real-time Monitoring**: Using `.status` and `.lastProgress`\n",
    "- **Trigger Modes**: `availableNow=True` for one-time batch processing\n",
    "\n",
    "#### 3. Medallion Architecture (Bronze ‚Üí Silver ‚Üí Gold)\n",
    "- **Bronze Zone**: Raw data preservation with audit metadata\n",
    "- **Silver Zone**: Cleaned, flattened, analytics-ready data\n",
    "- **Data Quality**: Understanding transformation at each layer\n",
    "- **Latency Monitoring**: Measuring end-to-end processing time\n",
    "\n",
    "#### 4. Production Pipeline Patterns\n",
    "- **Spark Job Definitions**: Long-running, orchestrated pipelines\n",
    "- **JSON Flattening**: Handling nested structures\n",
    "\n",
    "---\n",
    "\n",
    "### üé¨ Your Streaming Job Status\n",
    "\n",
    "**Remember:** Your Spark Job Definition (`StreamBronzeAndSilver`) is still running in the background!\n",
    "\n",
    "**Current State:**\n",
    "- Generating synthetic data every few seconds\n",
    "- Processing Landing ‚Üí Bronze ‚Üí Silver continuously\n",
    "- Data accumulating in Delta tables\n",
    "\n",
    "**What You Can Do:**\n",
    "- **Re-run queries** in this notebook to see growing data volumes\n",
    "- **Check the Lakehouse** to explore tables and schemas\n",
    "- **Stop the job** if needed (navigate to workspace ‚Üí Spark Job Definition ‚Üí Cancel)\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ What's Next?\n",
    "\n",
    "Continue your journey through the McMillan Industrial Group data pipeline:\n",
    "\n",
    "| Experience | What You'll Learn |\n",
    "|----------|-------------------|\n",
    "| **ü•á 2_GoldLayer** Notebook | Build dimensional models in Fabric Warehouse |\n",
    "| **ü§ñ 3_SalesAndLogisticsAgent** | Chat with your data via a Data Agent |\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "Expand your knowledge with these official docs:\n",
    "\n",
    "- [Microsoft Fabric Documentation](https://learn.microsoft.com/fabric/)\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Delta Lake Best Practices](https://docs.delta.io/latest/best-practices.html)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Pro Tips for Your Own Projects\n",
    "\n",
    "**When building streaming pipelines:**\n",
    "1. **Start with batch** - Prototype transformations in batch mode first\n",
    "2. **Monitor latency** - Track end-to-end processing times from the start\n",
    "3. **Test with sample data** - Validate logic before processing production volumes\n",
    "4. **Document schemas** - Maintain schema definitions for all layers\n",
    "5. **Plan for reprocessing** - Design Bronze so you can reprocess Silver at any time\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Great work completing this notebook!** Move on to the next notebook when you're ready to build the Gold layer! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
